{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Joey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "# Imports used in Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# general python imports i.e. working with dataframes and basic maths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import datetime as datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import ast as ast\n",
    "\n",
    "#dashboard imports\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table\n",
    "\n",
    "# used to get most common words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_score\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Import some text packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#folium doesn't seem like it works in dash, but really good code anyway\n",
    "import folium\n",
    "import os\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Joey\\\\Google Drive\\\\5. Python Programs\\\\Webscraping\\\\2.Unfinished\\\\Chicken Dash'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entering Postcodes and getting restaurant URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should cover all of greater london\n",
    "postcode_list=['E8 1EX', 'E17 4PP', 'E6 6AB', 'SE7 8BL', 'SE2 0XT',\n",
    "              'SW16 1BB', 'SW4 8EL', 'SW17 9NA', 'SW19 5AE', 'SW6 3PR', \n",
    "               'W5 5TH','NW8 9NH', 'NW6 1RZ','NW7 1NB','N8 8JD', \n",
    "               'E16 1XL', 'SE25 6AB', 'SE23 1NW', 'SE1 9SG','N21 1LE',\n",
    "              'SE10 8JQ','EC1V 4NB','SW1V 2BP','W6 7NL','TW8 8JF','TW9 1DN',\n",
    "              'BR1 3PX','N1 8AB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "#driver = webdriver.Chrome(executable_path='C:\\\\webdriver\\\\chromedriver.exe',chrome_options=options)\n",
    "\n",
    "def shop_grabber2(postcode_list):\n",
    "    driver = webdriver.Chrome(executable_path='C:\\\\webdriver\\\\chromedriver.exe',chrome_options=options)\n",
    "    \n",
    "    for i in tqdm(postcode_list):\n",
    "        \n",
    "        driver.get('https://www.just-eat.co.uk/')\n",
    "\n",
    "        post_field = driver.find_element_by_class_name(\"Form_input_y1xkO\")\n",
    "        post_field.clear()\n",
    "        #post_field.click()\n",
    "        post_field.send_keys(i)\n",
    "        find_retaurants=driver.find_element_by_class_name(\"Form_btn_TuG34\")\n",
    "        find_retaurants.click()\n",
    "\n",
    "        \n",
    "\n",
    "        for shop in driver.find_elements_by_class_name('c-listing-item-link'):\n",
    "            url=shop.get_attribute('href')\n",
    "            \n",
    "            try:\n",
    "                cusine=shop.find_element_by_class_name('c-listing-item-text').text\n",
    "            except:\n",
    "                cusine=None\n",
    "                \n",
    "            food_list2.append([url,cusine])\n",
    "\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning:\n",
      "\n",
      "use options instead of chrome_options\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01804ee070e043438e4d23d53dc6f85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c166f4ea4540ce9a094503bb7751fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6245"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_list2=[]\n",
    "\n",
    "\n",
    "post1=postcode_list[0:14]\n",
    "post2=postcode_list[14:28]\n",
    "#post3=postcode_list[20:28]\n",
    "# post4=postcode_list[21:28]\n",
    "\n",
    "\n",
    "jobs = []\n",
    "\n",
    "\n",
    "thread1 = threading.Thread(target=shop_grabber2, args=(post1,), )\n",
    "thread2 = threading.Thread(target=shop_grabber2, args=(post2,), )\n",
    "#thread3 = threading.Thread(target=shop_grabber2, args=(post3,), )\n",
    "# thread4 = threading.Thread(target=shop_grabber2, args=(post4,), )\n",
    "jobs.append(thread1)\n",
    "jobs.append(thread2)\n",
    "#jobs.append(thread3)\n",
    "# jobs.append(thread4)\n",
    "\n",
    "# Start the threads\n",
    "for j in jobs:\n",
    "    j.start()\n",
    "    \n",
    "# Ensure all of the threads have finished\n",
    "for j in jobs:\n",
    "    j.join()\n",
    "    \n",
    "len(food_list2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just eat restaurants in greater london= 3732\n"
     ]
    }
   ],
   "source": [
    "url_df=pd.DataFrame(food_list2,columns=['url','cuisine']).drop_duplicates(['url'],keep='first')\n",
    "url_df.to_csv('menu_urls.csv')\n",
    "\n",
    "print('just eat restaurants in greater london=',len(url_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df=pd.read_csv('C:\\\\Users\\\\Joey\\\\Google Drive\\\\5. Python Programs\\\\Webscraping\\\\2.Unfinished\\\\Chicken Dash\\\\menu_urls.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting each restaurant's postcode, name and URL for review page - potential to scrape menu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "shops=[]\n",
    "\n",
    "def shopgrabber3(restaurants):\n",
    "    driver = webdriver.Chrome(executable_path='C:\\\\webdriver\\\\chromedriver.exe',chrome_options=options)\n",
    "    \n",
    "    for i in tqdm(restaurants):\n",
    "        \n",
    "        driver.get(i)\n",
    "        \n",
    "        link_2_reviews=i.replace('/menu','/reviews')\n",
    "        \n",
    "        try:\n",
    "            name=driver.find_element_by_xpath('//*[@id=\"restaurant\"]/div[2]/div[1]/div[2]/h1').text\n",
    "        except:\n",
    "            try:\n",
    "                name=driver.find_element_by_xpath('//*[@id=\"restaurant\"]/div[2]/div[2]/div[2]/h1').text\n",
    "            except:\n",
    "                name=np.nan\n",
    "                #print('bullshit with name')\n",
    "                \n",
    "        try:\n",
    "            postcode=driver.find_element_by_xpath('//*[@id=\"postcode\"]').text\n",
    "        except:\n",
    "            postcode=np.nan\n",
    "            #print('bullshit with postcode')    \n",
    "        \n",
    "       \n",
    "        \n",
    "        shops.append([name,postcode,link_2_reviews,i])\n",
    "        \n",
    "    driver.quit()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3732"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8430592bbcb04edf9a6e154412eb5530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1866), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a114658755704f5090c4ef0bce3d987b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1866), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3732"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shops=[]\n",
    "\n",
    "list1=url_df.url[0:1866]\n",
    "list2=url_df.url[1866:3732]\n",
    "\n",
    "jobs = []\n",
    "\n",
    "thread1 = threading.Thread(target=shopgrabber3, args=(list1,), )\n",
    "thread2 = threading.Thread(target=shopgrabber3, args=(list2,), )\n",
    "\n",
    "jobs.append(thread1)\n",
    "jobs.append(thread2)\n",
    "\n",
    "# Start the threads\n",
    "for j in jobs:\n",
    "    j.start()\n",
    "    \n",
    "# Ensure all of the threads have finished\n",
    "for j in jobs:\n",
    "    j.join()\n",
    "    \n",
    "len(shops)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(shops,columns=['name','postcode','link_2_reviews','url']).merge(url_df,how='inner')\n",
    "df.to_csv('restaurants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run From Here When Refreshing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('restaurants',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pc_prefix\n",
       "N1      88\n",
       "SE1     80\n",
       "E17     80\n",
       "E1      78\n",
       "E2      75\n",
       "SW17    68\n",
       "NW6     68\n",
       "CR0     65\n",
       "SW11    60\n",
       "SW19    60\n",
       "SW16    56\n",
       "E14     53\n",
       "BR1     48\n",
       "W2      48\n",
       "SW18    46\n",
       "W12     45\n",
       "E13     45\n",
       "NW10    44\n",
       "SE18    44\n",
       "E8      44\n",
       "Name: postcode, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pc_prefix']=[str(x).split(' ')[0] for x in df.postcode]\n",
    "df.groupby(['pc_prefix']).count().sort_values('postcode',ascending=False)['postcode'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3732"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Ratings and Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "def review_getter(ziplist):    \n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path='C:\\\\webdriver\\\\chromedriver.exe',chrome_options=options)\n",
    "    \n",
    "    for i in tqdm(ziplist):\n",
    "        \n",
    "        driver.get(i[0])\n",
    "        try:\n",
    "            overall_rating=[]\n",
    "            total_reviews=[]\n",
    "\n",
    "            try:\n",
    "                overall_rating=float(driver.find_element_by_xpath('//*[@id=\"restaurant\"]/div[2]/div[2]/div[2]/span/span').text)\n",
    "                overall_rating=round((overall_rating/6)*100,1)\n",
    "                total_reviews=int(driver.find_element_by_xpath('//*[@id=\"restaurant\"]/div[2]/div[1]/div[2]/div/p/a').text.split(' ')[0])\n",
    "            except:\n",
    "                overall_rating=np.nan\n",
    "                total_reviews=np.nan\n",
    "\n",
    "\n",
    "            dates=[]\n",
    "            stars=[]\n",
    "            reviews=[]\n",
    "\n",
    "            #try:\n",
    "            for review in driver.find_element_by_class_name('restaurantRatings').find_elements_by_css_selector('li'):\n",
    "\n",
    "                #date\n",
    "                try:\n",
    "                    date=review.find_element_by_class_name('date').text\n",
    "                    date2=datetime.datetime.strptime(date,\"%d/%m/%Y\").date()\n",
    "                    dates.append(date2)\n",
    "                except:\n",
    "                    dates.append(np.nan)\n",
    "\n",
    "                #rating\n",
    "                try:\n",
    "                    rating=float(review.find_elements_by_css_selector('img')[0].get_attribute('title').split(' ')[0])\n",
    "                    rating2=round((rating/6)*100,1)\n",
    "                    stars.append(float(rating2))\n",
    "                except:\n",
    "                    stars.append(np.nan)\n",
    "\n",
    "                #review\n",
    "                try:\n",
    "                    #comment=review.text.split('\\n')[2]\n",
    "                    comment=review.find_element_by_class_name('comments').text\n",
    "                    reviews.append(comment)\n",
    "                except:\n",
    "                    reviews.append(None)\n",
    "    #         except:\n",
    "    #             dates=np.nan\n",
    "    #             stars=np.nan\n",
    "    #             reviews=np.nan\n",
    "\n",
    "            review_dict = {'overall_rating':overall_rating,\n",
    "                           'total_reviews':total_reviews,\n",
    "                            'date' : dates,\n",
    "                           'stars' : stars,\n",
    "                           'review_text' : reviews,\n",
    "                           'link_2_reviews' : i[0],\n",
    "                           'review_target': i[1]}\n",
    "\n",
    "\n",
    "            #reviews_df = pd.DataFrame(reviews_list)\n",
    "\n",
    "            review_df=pd.DataFrame(review_dict)\n",
    "\n",
    "            all_reviews.append(review_df)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    driver.quit()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933.0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning:\n",
      "\n",
      "use options instead of chrome_options\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=933), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=933), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=933), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=933), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ziplist=list(zip(df.link_2_reviews,df.name))\n",
    "\n",
    "all_reviews=[]\n",
    "\n",
    "threads=4\n",
    "x=int(round(len(df)/threads))\n",
    "remainder=int(((len(df)/threads)-x)*threads)\n",
    "\n",
    "list1=ziplist[0:x]\n",
    "list2=ziplist[x:(2*x)]\n",
    "list3=ziplist[(2*x):(3*x)]\n",
    "list4=ziplist[(3*x):(4*x)+remainder]\n",
    "\n",
    "\n",
    "jobs = []\n",
    "\n",
    "thread1 = threading.Thread(target=review_getter, args=(list1,), )\n",
    "thread2 = threading.Thread(target=review_getter, args=(list2,), )\n",
    "thread3 = threading.Thread(target=review_getter, args=(list3,), )\n",
    "thread4 = threading.Thread(target=review_getter, args=(list4,), )\n",
    "\n",
    "\n",
    "jobs.append(thread1)\n",
    "jobs.append(thread2)\n",
    "jobs.append(thread3)\n",
    "jobs.append(thread4)\n",
    "\n",
    "# Start the threads\n",
    "for j in jobs:\n",
    "    j.start()\n",
    "    \n",
    "# Ensure all of the threads have finished\n",
    "for j in jobs:\n",
    "    j.join()\n",
    "    \n",
    "len(all_reviews)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-44:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: review_getter() missing 1 required positional argument: 'ziplist'\n",
      "Exception in thread Thread-45:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: review_getter() takes 1 positional argument but 933 were given\n",
      "Exception in thread Thread-46:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "TypeError: review_getter() takes 1 positional argument but 933 were given\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jobs=[]\n",
    "\n",
    "def jobber(thread_count,dataframe):\n",
    "\n",
    "    threads=thread_count\n",
    "    x=int(round(len(dataframe)/threads))\n",
    "    remainder=int(((len(dataframe)/threads)-x)*thread_count)\n",
    "    \n",
    "    for i in range(threads-1):\n",
    "        jobs.append(threading.Thread(target=review_getter, args=(ziplist[(i-1)*x:(i*x)])))\n",
    "        \n",
    "    #jobs.append(threading.Thread(target=review_getter, args=(ziplist[((thread_count-1)*x):(thread_count*x)+remainder])))\n",
    "        \n",
    "    return \n",
    "\n",
    "jobber(4,df)\n",
    "\n",
    "# Start the threads\n",
    "for j in jobs:\n",
    "    j.start()\n",
    "    \n",
    "# Ensure all of the threads have finished\n",
    "for j in jobs:\n",
    "    j.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can apply NLP to test \n",
    "test=pd.concat(all_reviews).reset_index().drop(columns=['index'])\n",
    "test['date_range']=test.date\n",
    "\n",
    "test2=test.groupby('link_2_reviews').aggregate({'overall_rating':'max',\n",
    "                                               'total_reviews':'max',\n",
    "                                               'date':'max',\n",
    "                                               'date_range': lambda x: max(x) - min(x),\n",
    "                                               'stars':'mean',\n",
    "                                               'review_text':'count',\n",
    "                                               'review_target':'first'}).reset_index()\n",
    "\n",
    "df2=test2.merge(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.rename(columns={'date':'most_recent_review','review_text':'reviews','stars':'trending_score'})\n",
    "df2.trending_score=df2.trending_score.apply(lambda x: round(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2353"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=df2[(df2.total_reviews>50) &\n",
    "        (df2.most_recent_review>datetime.date(2019, 3, 1))]\n",
    "\n",
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Burgers': 318,\n",
       "         'Fast': 30,\n",
       "         'Food': 34,\n",
       "         'Halal': 1458,\n",
       "         'African': 50,\n",
       "         'Nigerian': 18,\n",
       "         'Caribbean': 125,\n",
       "         'English': 24,\n",
       "         'Pizza': 745,\n",
       "         'Italian': 415,\n",
       "         'Gourmet': 28,\n",
       "         'Breakfast': 62,\n",
       "         'Grill': 174,\n",
       "         'Fish': 105,\n",
       "         '&': 105,\n",
       "         'Chips': 105,\n",
       "         'Korean': 10,\n",
       "         'Asian': 37,\n",
       "         'Kebab': 299,\n",
       "         'Indian': 740,\n",
       "         'Fusion': 8,\n",
       "         'Sushi': 165,\n",
       "         'Chinese': 453,\n",
       "         'Turkish': 129,\n",
       "         'Milkshakes': 9,\n",
       "         'American': 207,\n",
       "         'Oriental': 260,\n",
       "         'Lebanese': 110,\n",
       "         'Middle': 34,\n",
       "         'Eastern': 38,\n",
       "         'Curry': 430,\n",
       "         'Thai': 171,\n",
       "         'South': 25,\n",
       "         'Jamaican': 64,\n",
       "         'Desserts': 81,\n",
       "         'Dim': 11,\n",
       "         'Sum': 11,\n",
       "         'Seafood': 7,\n",
       "         'Bangladeshi': 64,\n",
       "         'Drinks': 14,\n",
       "         'Peri': 238,\n",
       "         'Chicken': 359,\n",
       "         'Japanese': 203,\n",
       "         'Pasta': 29,\n",
       "         'Sri-lankan': 10,\n",
       "         'Syrian': 4,\n",
       "         'Mediterranean': 78,\n",
       "         'Ethiopian': 4,\n",
       "         'Vegan': 40,\n",
       "         'British': 47,\n",
       "         'Vegetarian': 39,\n",
       "         'Afghan': 8,\n",
       "         'Sweets': 4,\n",
       "         'Smoothies': 5,\n",
       "         'Ice': 23,\n",
       "         'Cream': 23,\n",
       "         'Malaysian': 12,\n",
       "         'Pakistani': 38,\n",
       "         'Wraps': 5,\n",
       "         'Egyptian': 2,\n",
       "         'Greek': 16,\n",
       "         'Café': 25,\n",
       "         'Arabic': 6,\n",
       "         'Moroccan': 7,\n",
       "         'Healthy': 37,\n",
       "         'Romanian': 4,\n",
       "         'European': 13,\n",
       "         'Cantonese': 2,\n",
       "         'Persian': 36,\n",
       "         'Portuguese': 16,\n",
       "         'Polish': 3,\n",
       "         'Azerbaijan': 1,\n",
       "         'Russian': 2,\n",
       "         'Bagels': 6,\n",
       "         'Sandwiches': 72,\n",
       "         'Vietnamese': 30,\n",
       "         '*London': 1,\n",
       "         'Loves*': 1,\n",
       "         'French': 6,\n",
       "         'Mexican': 30,\n",
       "         'Burritos': 6,\n",
       "         'BBQ': 6,\n",
       "         'Iranian': 15,\n",
       "         'Noodles': 42,\n",
       "         'Spanish': 7,\n",
       "         'Brazilian': 7,\n",
       "         'food': 7,\n",
       "         'Taiwanese': 1,\n",
       "         'Lunch': 11,\n",
       "         'North': 3,\n",
       "         'Cakes': 12,\n",
       "         \"Panini's\": 2,\n",
       "         'Tex': 1,\n",
       "         'Mex': 1,\n",
       "         'Street': 4,\n",
       "         'Salads': 7,\n",
       "         'Nepalese': 31,\n",
       "         'Balti': 3,\n",
       "         'Biryani': 6,\n",
       "         'Waffles': 1,\n",
       "         'Continental': 8,\n",
       "         'Hot': 2,\n",
       "         'Dogs': 2,\n",
       "         'Jerk': 4,\n",
       "         'Tapas': 1,\n",
       "         'Steak': 7,\n",
       "         'Latin': 4,\n",
       "         'Indonesian': 1,\n",
       "         'Filipino': 2,\n",
       "         'Dinner': 1,\n",
       "         'Bubble': 1,\n",
       "         'Tea': 1,\n",
       "         'Punjabi': 2,\n",
       "         'Gluten': 1,\n",
       "         'Free': 1,\n",
       "         'Peruvian': 1,\n",
       "         'Ukrainian': 1,\n",
       "         'Singapore': 2,\n",
       "         'Kurdish': 1,\n",
       "         'Subways': 47,\n",
       "         'Kosher': 1,\n",
       "         'Ghanaian': 1,\n",
       "         'Deli': 1,\n",
       "         'Crepes': 2,\n",
       "         'Hungarian': 1,\n",
       "         'Roast': 2,\n",
       "         'Dinners': 2})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_lists=[x.split(' ') for x in df2.cuisine]\n",
    "flat_list = [item for sublist in list_lists for item in sublist]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Counter(flat_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional data prep\n",
    "\n",
    "df3=df3.dropna()\n",
    "\n",
    "chains=['kfc' , 'burger king' , 'subway' , 'pizza hut' , 'papa j']\n",
    "\n",
    "df3['corporation']=['chain' if any(thing in x.lower() for thing in chains) else 'independent' for x in df3.name]\n",
    "      \n",
    "df3[df3.total_reviews>50].to_csv('test.csv')\n",
    "\n",
    "postcodes=[]\n",
    "for thing in df3.pc_prefix:\n",
    "    if 'BR' in thing:\n",
    "        a='BR'\n",
    "    elif 'DA' in thing:\n",
    "        a='DA'\n",
    "    elif 'HA' in thing:\n",
    "        a='HA'\n",
    "    elif 'EN' in thing:\n",
    "        a='EN'\n",
    "    elif 'IG' in thing:\n",
    "        a='IG'\n",
    "    elif 'KT' in thing:\n",
    "        a='KT'\n",
    "    elif 'SM' in thing:\n",
    "        a='SM'\n",
    "    elif 'TW' in thing:\n",
    "        a='TW' \n",
    "    elif 'UB' in thing:\n",
    "        a='UB'\n",
    "    elif 'WC1' in thing:\n",
    "        a='WC1'    \n",
    "    elif 'WC2' in thing:\n",
    "        a='WC2'\n",
    "    elif 'EC1' in thing:\n",
    "        a='EC1'        \n",
    "    elif 'EC2' in thing:\n",
    "        a='EC2'        \n",
    "    elif 'EC3' in thing:\n",
    "        a='EC3'        \n",
    "    elif 'EC4' in thing:\n",
    "        a='EC4' \n",
    "    else:\n",
    "        a=thing\n",
    "    postcodes.append(a)\n",
    "        \n",
    "\n",
    "df3.pc_prefix=postcodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "strata=pd.read_csv('C:\\\\Users\\\\Joey\\\\Desktop\\\\uswitch postcode data\\\\190122 Postcode Level - MASTER.csv')\n",
    "\n",
    "df3['postcode_clean']=df3.postcode.apply(lambda x: x.replace(' ',''))\n",
    "df3=df3.merge(strata[['postcode_clean','latitude','longitude']],how='left')\n",
    "df3.to_csv('takeaway_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dash Code - Run in Anaconda Prompt for preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using bootstrap\n",
    "\n",
    "import numpy as np\n",
    "import dash\n",
    "import dash_table\n",
    "import dash_core_components as dcc\n",
    "import dash_bootstrap_components as dbc\n",
    "import dash_html_components as html\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import base64\n",
    "\n",
    "mapbox_access_token = 'pk.eyJ1Ijoiam9leTkxIiwiYSI6ImNqdDA3emFzazA5Y2IzeW56ZHRtN3Q3NG0ifQ.0l4idLnp8xRT8Y_Ipyj7vg'\n",
    "\n",
    "#dataframe for maps and table\n",
    "table_df = pd.read_csv('C:\\\\Users\\\\Joey\\\\Google Drive\\\\5. Python Programs\\\\Webscraping\\\\2.Unfinished\\\\Chicken Dash\\\\takeaway_data.csv',index_col=0)\n",
    "table_df=table_df.rename(columns={'total_reviews':'Reviews ','trending_score':'Trending Score ','overall_rating':'Lifetime Score '})\n",
    "\n",
    "table_df = table_df.sort_values('Trending Score ',ascending=False)\n",
    "table_df=table_df.dropna()\n",
    "\n",
    "zipped=zip(table_df.name\n",
    "           ,table_df.postcode\n",
    "           ,table_df['Lifetime Score '])\n",
    "\n",
    "table_df['text']=[x +', '+y+', Rating: '+str(z) for x,y,z in zipped]\n",
    "\n",
    "#for cuisine selector\n",
    "acceptable=['Chicken','Pizza','Chinese','Italian','Caribbean','Burgers','Kebab','Sandwiches','British','Thai',\n",
    "           'Vietnamese','Japanese','African','Indian','Curry','Mexican','Mediterranean']\n",
    "\n",
    "\n",
    "#for bar chart \n",
    "acceptable2=['Chicken','Pizza','Burgers','Sandwiches','Chinese','Indian','Kebab','British','Italian','Caribbean','Thai',\n",
    "           'Vietnamese','Japanese']\n",
    "\n",
    "cuisine=[]\n",
    "average_rating=[]\n",
    "volume=[]\n",
    "restaurant_type=[]\n",
    "\n",
    "    \n",
    "#Human Theory Logo\n",
    "image_filename = 'C:\\\\Users\\\\Joey\\\\Google Drive\\\\5. Python Programs\\\\Webscraping\\\\2.Unfinished\\\\Chicken Dash\\\\Human-Theory-Typemark-Sunset-Orange_RGB.png'\n",
    "encoded_image = base64.b64encode(open(image_filename, 'rb').read())\n",
    "\n",
    "\n",
    "#top of site navigation bar\n",
    "\n",
    "navbar = dbc.NavbarSimple(\n",
    "    html.A([\n",
    "            html.Img(src='data:image/png;base64,{}'.format(encoded_image.decode()),height='70', \n",
    "             width='140')\n",
    "        ], href='https://www.humantheory.co.uk'),\n",
    "    \n",
    "#     brand=\"Human Theory\",\n",
    "#     brand_href=\"https://www.humantheory.co.uk/\",\n",
    "    sticky=\"top\",\n",
    ")\n",
    "\n",
    "\n",
    "#2 containers in one row\n",
    "\n",
    "body = dbc.Container(\n",
    "    [\n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(\n",
    "                    [\n",
    "                        html.H2(\"Best in your area\"),\n",
    "                        dcc.Dropdown(id='postSelector',\n",
    "                                     options=[{'label': i, 'value': i} for i in sorted(table_df.pc_prefix.unique())],\n",
    "                                     value='E8',\n",
    "                                     style={'width': '90%'}),\n",
    "                        \n",
    "                       dcc.Dropdown(id='cuisineSelector2',\n",
    "                                    options=[{'label': i, 'value': i} for i in sorted(acceptable)],\n",
    "                                    value='Chicken',\n",
    "                                    style={'width': '90%'}),\n",
    "                        \n",
    "                        html.Div(id='table-container')\n",
    "                    ],\n",
    "                    md=4,\n",
    "                ),\n",
    "                dbc.Col(\n",
    "                    [\n",
    "                       html.H2(\"Takeaway Map of London\"),\n",
    "                        \n",
    "                       dcc.Dropdown(id='cuisineSelector',\n",
    "                                    options=[{'label': i, 'value': i} for i in sorted(acceptable)],\n",
    "                                    value='Chicken',\n",
    "                                    style={'width': '48%'}),\n",
    "                        \n",
    "                       dcc.RadioItems(id='corpSelector',\n",
    "                                      options=[{'label': i, 'value': i} for i in ['chain', 'independent']],\n",
    "                                      value='independent',\n",
    "                                      labelStyle={'display': 'inline-block'}), \n",
    "                        \n",
    "                      dcc.Graph(id='area-shops')\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    className=\"mt-4\",\n",
    ")\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.CERULEAN])\n",
    "\n",
    "app.layout = html.Div([navbar, body])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#callbacks\n",
    "\n",
    "def generate_table(dataframe):\n",
    "    return html.Table(\n",
    "        # Header\n",
    "        [html.Tr([html.Th(col) for col in dataframe.columns])] +\n",
    "\n",
    "        # Body\n",
    "        [html.Tr([\n",
    "        html.Td(dataframe.iloc[i][col]) for col in dataframe.columns\n",
    "        ]) for i in range(len(dataframe))]\n",
    "    )\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('table-container', 'children'),\n",
    "    [dash.dependencies.Input('postSelector', 'value'),\n",
    "     dash.dependencies.Input('cuisineSelector2', 'value')])\n",
    "\n",
    "def display_table(postSelector,cuisineSelector2):\n",
    "\n",
    "    #df = table_df[(table_df.pc_prefix==postSelector) ][['name',  'Trending Score ', 'Lifetime Score ','Reviews ']].head(10)\n",
    "    \n",
    "    df = table_df[(table_df.pc_prefix==postSelector) & \n",
    "                  (table_df.cuisine.str.contains(cuisineSelector2, regex=False))][['name',  'Trending Score ', 'Lifetime Score ','Reviews ']].head(10)\n",
    "\n",
    "    return generate_table(df)\n",
    "\n",
    "\n",
    "\n",
    "#map callback\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('area-shops', 'figure'),\n",
    "    [#dash.dependencies.Input('postSelector', 'value'),\n",
    "    dash.dependencies.Input('cuisineSelector', 'value'),\n",
    "    dash.dependencies.Input('corpSelector', 'value')\n",
    "    ])\n",
    "\n",
    "def update_graph(cuisineSelector='Chicken',corpSelector='independent'):\n",
    "\n",
    "    \n",
    "    df=table_df[#(table_df.pc_prefix==postSelector) & \n",
    "                (table_df.cuisine.str.contains(cuisineSelector, regex=False)) \n",
    "                & (table_df.corporation==corpSelector)\n",
    "    ]\n",
    "\n",
    "    #red points\n",
    "    data = [\n",
    "        go.Scattermapbox(\n",
    "            lat=df[df['Lifetime Score ']<=60]['latitude'],\n",
    "            lon=df[df['Lifetime Score ']<=60]['longitude'],\n",
    "            text = df[df['Lifetime Score ']<=60]['text'],\n",
    "            mode='markers',\n",
    "            marker=dict(size= 13,color = 'red',opacity = 1),\n",
    "            name='shit food',\n",
    "            showlegend=False\n",
    "        ),\n",
    "\n",
    "    #light red\n",
    "        go.Scattermapbox(\n",
    "            lat=df[(df['Lifetime Score ']>60) & (df['Lifetime Score ']<=70)]['latitude'],\n",
    "            lon=df[(df['Lifetime Score ']>60) & (df['Lifetime Score ']<=70)]['longitude'],\n",
    "            text = df[(df['Lifetime Score ']>60) & (df['Lifetime Score ']<=70)]['text'],\n",
    "            mode='markers',\n",
    "            marker=dict(size= 13,color = 'lightsalmon',opacity = 1),\n",
    "            name='bad food',\n",
    "            showlegend=False),\n",
    "\n",
    "\n",
    "    #lawn green\n",
    "\n",
    "        go.Scattermapbox(\n",
    "            lat=df[(df['Lifetime Score ']>70) & (df['Lifetime Score ']<=80)]['latitude'],\n",
    "            lon=df[(df['Lifetime Score ']>70) & (df['Lifetime Score ']<=80)]['longitude'],\n",
    "            text = df[(df['Lifetime Score ']>70) & (df['Lifetime Score ']<=80)]['text'],\n",
    "            mode='markers',\n",
    "            marker=dict(size= 13,color = 'lawngreen',opacity = 1),\n",
    "            name='good food',\n",
    "            showlegend=False),\n",
    "\n",
    "    #green\n",
    "\n",
    "        go.Scattermapbox(\n",
    "            lat=df[df['Lifetime Score ']>80]['latitude'],\n",
    "            lon=df[df['Lifetime Score ']>80]['longitude'],\n",
    "            text = df[df['Lifetime Score ']>80]['text'],\n",
    "            mode='markers',\n",
    "            marker=dict(size= 13,color = 'green',opacity = 1),\n",
    "            name='excellent food',\n",
    "            showlegend=False),\n",
    "    ]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        autosize=False,\n",
    "        width=700,\n",
    "        height=450,\n",
    "        hovermode='closest',\n",
    "        margin=go.layout.Margin(\n",
    "        l=1,\n",
    "        r=1,\n",
    "         b=20,\n",
    "         t=1,\n",
    "#         pad=4\n",
    "    ),\n",
    "        mapbox=go.layout.Mapbox(accesstoken=mapbox_access_token,\n",
    "                                bearing=0,\n",
    "                                center=go.layout.mapbox.Center(lat=51.512324, lon=-0.075366),\n",
    "                                pitch=0,\n",
    "                                zoom=10),\n",
    "    )\n",
    "\n",
    "    return go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
