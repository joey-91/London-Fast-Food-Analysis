{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Joey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# Imports used in Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# general python imports i.e. working with dataframes and basic maths\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import datetime as datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import ast as ast\n",
    "\n",
    "#dashboard imports\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table\n",
    "\n",
    "# used to get most common words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_score\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Import some text packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# sentiment analysis\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#folium doesn't seem like it works in dash, but really good code anyway\n",
    "import folium\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import threading\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Joey\\\\Google Drive\\\\5. Python Programs\\\\Webscraping\\\\2.Unfinished\\\\Chicken Dash'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entering Postcodes and getting restaurant URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should cover all of greater london\n",
    "postcode_list=['E8 1EX', 'E17 4PP', 'E6 6AB', 'SE7 8BL', 'SE2 0XT',\n",
    "              'SW16 1BB', 'SW4 8EL', 'SW17 9NA', 'SW19 5AE', 'SW6 3PR', \n",
    "               'W5 5TH','NW8 9NH', 'NW6 1RZ','NW7 1NB','N8 8JD', \n",
    "               'E16 1XL', 'SE25 6AB', 'SE23 1NW', 'SE1 9SG','N21 1LE',\n",
    "              'SE10 8JQ','EC1V 4NB','SW1V 2BP','W6 7NL','TW8 8JF','TW9 1DN',\n",
    "              'BR1 3PX','N1 8AB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "#driver = webdriver.Chrome(executable_path='C:\\\\webdriver\\\\chromedriver.exe',chrome_options=options)\n",
    "\n",
    "def shop_grabber2(postcode_list):\n",
    "    driver = webdriver.Chrome(executable_path='C:\\\\Users\\\\Joey\\\\Desktop\\\\chromedriver.exe',chrome_options=options)\n",
    "    \n",
    "    for i in tqdm(postcode_list):\n",
    "        \n",
    "        driver.get('https://www.just-eat.co.uk/')\n",
    "        \n",
    "        post_field = driver.find_element_by_class_name(\"Form_c-search-input_2hlRP\")\n",
    "        \n",
    "        post_field.clear()\n",
    "        #post_field.click()\n",
    "        post_field.send_keys(i)\n",
    "        find_retaurants=driver.find_element_by_class_name(\"Form_c-search-btn_1EEhL\")\n",
    "        find_retaurants.click()\n",
    "\n",
    "        \n",
    "\n",
    "        for shop in driver.find_elements_by_class_name('c-listing-item-link'):\n",
    "            url=shop.get_attribute('href')\n",
    "            \n",
    "            try:\n",
    "                cusine=shop.find_element_by_class_name('c-listing-item-text').text\n",
    "            except:\n",
    "                cusine=None\n",
    "                \n",
    "            food_list2.append([url,cusine])\n",
    "\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning:\n",
      "\n",
      "use options instead of chrome_options\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeca6bee3e5e40c3bb159d6545c13b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d4614f5db24b8bbbdd1f31f3a57316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "6958"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_list2=[]\n",
    "\n",
    "\n",
    "post1=postcode_list[0:14]\n",
    "post2=postcode_list[14:28]\n",
    "#post3=postcode_list[20:28]\n",
    "# post4=postcode_list[21:28]\n",
    "\n",
    "\n",
    "jobs = []\n",
    "\n",
    "\n",
    "thread1 = threading.Thread(target=shop_grabber2, args=(post1,), )\n",
    "thread2 = threading.Thread(target=shop_grabber2, args=(post2,), )\n",
    "#thread3 = threading.Thread(target=shop_grabber2, args=(post3,), )\n",
    "# thread4 = threading.Thread(target=shop_grabber2, args=(post4,), )\n",
    "jobs.append(thread1)\n",
    "jobs.append(thread2)\n",
    "#jobs.append(thread3)\n",
    "# jobs.append(thread4)\n",
    "\n",
    "# Start the threads\n",
    "for j in jobs:\n",
    "    j.start()\n",
    "    \n",
    "# Ensure all of the threads have finished\n",
    "for j in jobs:\n",
    "    j.join()\n",
    "    \n",
    "len(food_list2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just eat restaurants in greater london= 3921\n"
     ]
    }
   ],
   "source": [
    "url_df=pd.DataFrame(food_list2,columns=['url','cuisine']).drop_duplicates(['url'],keep='first')\n",
    "url_df.to_csv('menu_urls.csv')\n",
    "\n",
    "print('just eat restaurants in greater london=',len(url_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df=pd.read_csv('C:\\\\Users\\\\Joey\\\\Google Drive\\\\5. Python Programs\\\\Webscraping\\\\2.Unfinished\\\\Chicken Dash\\\\menu_urls.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting each restaurant's postcode, name and URL for review page - potential to scrape menu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "shops=[]\n",
    "\n",
    "def shopgrabber3(restaurants):\n",
    "    driver = webdriver.Chrome(executable_path='C:\\\\Users\\\\Joey\\\\Desktop\\\\chromedriver.exe',chrome_options=options)\n",
    "    \n",
    "    for i in tqdm(restaurants):\n",
    "        \n",
    "        driver.get(i)\n",
    "        \n",
    "        link_2_reviews=i.replace('/menu','/reviews')\n",
    "        \n",
    "        try:\n",
    "            name=driver.find_element_by_xpath('//*[@id=\"restaurant\"]/div[2]/div[1]/div[2]/h1').text\n",
    "        except:\n",
    "            try:\n",
    "                name=driver.find_element_by_xpath('//*[@id=\"restaurant\"]/div[2]/div[2]/div[2]/h1').text\n",
    "            except:\n",
    "                name=np.nan\n",
    "                #print('bullshit with name')\n",
    "                \n",
    "        try:\n",
    "            postcode=driver.find_element_by_xpath('//*[@id=\"postcode\"]').text\n",
    "        except:\n",
    "            postcode=np.nan\n",
    "            #print('bullshit with postcode')    \n",
    "        \n",
    "       \n",
    "        \n",
    "        shops.append([name,postcode,link_2_reviews,i])\n",
    "        \n",
    "    driver.quit()\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3921"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning:\n",
      "\n",
      "use options instead of chrome_options\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3215c37854f24e038b556d4695178ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1960), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155d0e624f26490b9a28e3aca5d1e125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2055), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4015"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shops=[]\n",
    "\n",
    "list1=url_df.url[0:1960]\n",
    "list2=url_df.url[1866:3921]\n",
    "\n",
    "jobs = []\n",
    "\n",
    "thread1 = threading.Thread(target=shopgrabber3, args=(list1,), )\n",
    "thread2 = threading.Thread(target=shopgrabber3, args=(list2,), )\n",
    "\n",
    "jobs.append(thread1)\n",
    "jobs.append(thread2)\n",
    "\n",
    "# Start the threads\n",
    "for j in jobs:\n",
    "    j.start()\n",
    "    \n",
    "# Ensure all of the threads have finished\n",
    "for j in jobs:\n",
    "    j.join()\n",
    "    \n",
    "len(shops)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(shops,columns=['name','postcode','link_2_reviews','url']).merge(url_df,how='inner')\n",
    "df['pc_prefix']=[str(x).split(' ')[0] for x in df.postcode]\n",
    "df.to_csv('restaurants')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run From Here When Refreshing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('restaurants',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Ratings and Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path='C:\\\\Users\\\\Joey\\\\Desktop\\\\chromedriver.exe')\n",
    "driver.get('https://www.just-eat.co.uk/restaurants-telepizza-hornsey/reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning:\n",
      "\n",
      "use options instead of chrome_options\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adf8dac5ccc4e7ea2e19c7da182e925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_getter([['https://www.just-eat.co.uk/restaurants-telepizza-hornsey/reviews','Telepizza']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "\n",
    "def review_getter(ziplist):    \n",
    "    \n",
    "    driver = webdriver.Chrome(executable_path='C:\\\\Users\\\\Joey\\\\Desktop\\\\chromedriver.exe'\n",
    "                              ,chrome_options=options\n",
    "                             )\n",
    "    \n",
    "    for i in tqdm(ziplist):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            driver.get(i[0])\n",
    "            overall_rating=[]\n",
    "            total_reviews=[]\n",
    "\n",
    "            try:\n",
    "                overall_rating=float(driver.find_element_by_xpath('//*[@id=\"restaurant\"]/div[2]/div[2]/div[2]/span/span').text)\n",
    "                overall_rating=round((overall_rating/6)*100,1)\n",
    "                total_reviews=int(driver.find_element_by_xpath('//*[@id=\"restaurant\"]/div[2]/div[1]/div[2]/div/p/a').text.split(' ')[0])\n",
    "            except:\n",
    "                overall_rating=np.nan\n",
    "                total_reviews=np.nan\n",
    "\n",
    "\n",
    "            dates=[]\n",
    "            stars=[]\n",
    "            reviews=[]\n",
    "\n",
    "            #try:\n",
    "            for review in driver.find_element_by_class_name('restaurantRatings').find_elements_by_css_selector('li'):\n",
    "\n",
    "                #date\n",
    "                try:\n",
    "                    date=review.find_element_by_class_name('date').text\n",
    "                    date2=datetime.datetime.strptime(date,\"%d/%m/%Y\").date()\n",
    "                    dates.append(date2)\n",
    "                except:\n",
    "                    dates.append(np.nan)\n",
    "\n",
    "                #rating\n",
    "                try:\n",
    "                    rating=float(review.find_elements_by_css_selector('img')[0].get_attribute('title').split(' ')[0])\n",
    "                    rating2=round((rating/6)*100,1)\n",
    "                    stars.append(float(rating2))\n",
    "                except:\n",
    "                    stars.append(np.nan)\n",
    "\n",
    "                #review\n",
    "                try:\n",
    "                    #comment=review.text.split('\\n')[2]\n",
    "                    comment=review.find_element_by_class_name('comments').text\n",
    "                    reviews.append(comment)\n",
    "                except:\n",
    "                    reviews.append(None)\n",
    "    #         except:\n",
    "    #             dates=np.nan\n",
    "    #             stars=np.nan\n",
    "    #             reviews=np.nan\n",
    "\n",
    "            review_dict = {'overall_rating':overall_rating,\n",
    "                           'total_reviews':total_reviews,\n",
    "                            'date' : dates,\n",
    "                           'stars' : stars,\n",
    "                           'review_text' : reviews,\n",
    "                           'link_2_reviews' : i[0],\n",
    "                           'review_target': i[1]}\n",
    "\n",
    "\n",
    "            #reviews_df = pd.DataFrame(reviews_list)\n",
    "\n",
    "            review_df=pd.DataFrame(review_dict)\n",
    "\n",
    "            all_reviews.append(review_df)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    driver.quit()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziplist=list(zip(df.link_2_reviews,df.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joey\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning:\n",
      "\n",
      "use options instead of chrome_options\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec3c36f532d48808c566b7bca49e9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=803), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6095adcfc13f44228a359db8f12ca78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=803), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976b6941c59249de81cea27521f253a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=803), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8398b6e881224815998abf161839cb5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c561801a247d49a4a97c4210ede5ba7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=803), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9ed5520a36434f87b3bf7f2f12fa48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=803), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_reviews=[]\n",
    "\n",
    "#Enter number of threads you want to try, and a list or df- to take the length of. and the name of your webscraping function..with no brackets or arguments\n",
    "def jobber(thread_count,iteration_list,webscrape_func):\n",
    "\n",
    "    jobs=[]\n",
    "    \n",
    "    #dividing the batch by number of threads\n",
    "    x=int(round(len(iteration_list)/thread_count))\n",
    "    remainder=int(((len(iteration_list)/thread_count)-x)*thread_count)\n",
    "    \n",
    "    for i in range(thread_count):\n",
    "        #appending the first n-1 jobs to the job list.. if i=1 and x =25 then i'm appending list[0:25]\n",
    "        jobs.append(threading.Thread(target=webscrape_func, \n",
    "                                     args=[iteration_list[(i-1)*x:(i*x)]]))\n",
    "    \n",
    "    #appending the last  job with remainder also added to the list\n",
    "    jobs.append(threading.Thread(target=webscrape_func, \n",
    "                                 args=[iteration_list[((thread_count-1)*x):(thread_count*x)+remainder]]))\n",
    "\n",
    "    \n",
    "    # Start the threads\n",
    "    for j in jobs:\n",
    "        j.start()\n",
    "    \n",
    "    # Ensure all of the threads have finished\n",
    "    for j in jobs:\n",
    "        j.join()\n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "jobber(5,ziplist,review_getter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.concat(all_reviews).reset_index().drop(columns=['index'])\n",
    "test.to_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can apply NLP to test \n",
    "test=pd.read_csv('reviews.csv')\n",
    "test['date_range']=test.date\n",
    "\n",
    "test2=test.groupby('link_2_reviews').aggregate({'overall_rating':'max',\n",
    "                                               'total_reviews':'max',\n",
    "                                               'date':'max',\n",
    "                                               #'date_range': lambda x: max(x) - min(x),\n",
    "                                               'stars':'mean',\n",
    "                                               'review_text':'count',\n",
    "                                               'review_target':'first'}).reset_index()\n",
    "\n",
    "df2=test2.merge(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.rename(columns={'date':'most_recent_review','review_text':'reviews','stars':'trending_score'})\n",
    "df2.trending_score=df2.trending_score.apply(lambda x: round(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3857 entries, 0 to 3856\n",
      "Data columns (total 12 columns):\n",
      "link_2_reviews        3857 non-null object\n",
      "overall_rating        3838 non-null float64\n",
      "total_reviews         3838 non-null float64\n",
      "most_recent_review    3857 non-null object\n",
      "trending_score        3857 non-null float64\n",
      "reviews               3857 non-null int64\n",
      "review_target         3819 non-null object\n",
      "name                  3819 non-null object\n",
      "postcode              3823 non-null object\n",
      "url                   3857 non-null object\n",
      "cuisine               3857 non-null object\n",
      "pc_prefix             3823 non-null object\n",
      "dtypes: float64(3), int64(1), object(8)\n",
      "memory usage: 391.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(string):\n",
    "    y=int(string.split('-')[0])\n",
    "    m=int(string.split('-')[1])\n",
    "    d=int(string.split('-')[2])\n",
    "    return datetime.date(y,m,d)\n",
    "\n",
    "df2.most_recent_review=df2.most_recent_review.apply(to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2465"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3=df2[(df2.total_reviews>50) &\n",
    "        (df2.most_recent_review>datetime.date(2019, 3, 1))]\n",
    "\n",
    "len(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Burgers': 341,\n",
       "         'Fast': 29,\n",
       "         'Food': 33,\n",
       "         'Halal': 1529,\n",
       "         'African': 53,\n",
       "         'Nigerian': 19,\n",
       "         'Caribbean': 132,\n",
       "         'English': 25,\n",
       "         'Pizza': 782,\n",
       "         'Italian': 448,\n",
       "         'Gourmet': 30,\n",
       "         'Breakfast': 68,\n",
       "         'Grill': 188,\n",
       "         'Korean': 12,\n",
       "         'Asian': 44,\n",
       "         'Kebab': 313,\n",
       "         'Indian': 765,\n",
       "         'Fusion': 10,\n",
       "         'Sushi': 181,\n",
       "         'Chinese': 478,\n",
       "         'Chicken': 391,\n",
       "         'Turkish': 141,\n",
       "         'Milkshakes': 8,\n",
       "         'American': 222,\n",
       "         'Lebanese': 115,\n",
       "         'Middle': 38,\n",
       "         'Eastern': 43,\n",
       "         'Curry': 438,\n",
       "         'Thai': 181,\n",
       "         'Oriental': 270,\n",
       "         'South': 26,\n",
       "         'Jamaican': 65,\n",
       "         'Desserts': 98,\n",
       "         'Dim': 13,\n",
       "         'Sum': 13,\n",
       "         'Seafood': 7,\n",
       "         'Bangladeshi': 67,\n",
       "         'Drinks': 15,\n",
       "         'Peri': 242,\n",
       "         'Fish': 112,\n",
       "         '&': 112,\n",
       "         'Chips': 112,\n",
       "         'Japanese': 224,\n",
       "         'Pasta': 29,\n",
       "         'Sri-lankan': 11,\n",
       "         'Syrian': 4,\n",
       "         'Mediterranean': 82,\n",
       "         'Ethiopian': 5,\n",
       "         'Vegan': 40,\n",
       "         'British': 51,\n",
       "         'Afghan': 9,\n",
       "         'Sweets': 4,\n",
       "         'Smoothies': 7,\n",
       "         'Ice': 29,\n",
       "         'Cream': 29,\n",
       "         'Malaysian': 13,\n",
       "         'Pakistani': 38,\n",
       "         'Wraps': 4,\n",
       "         'Egyptian': 2,\n",
       "         'Greek': 16,\n",
       "         'Arabic': 8,\n",
       "         'CafÃ©': 27,\n",
       "         'Mexican': 38,\n",
       "         'Moroccan': 8,\n",
       "         'Healthy': 42,\n",
       "         'Romanian': 4,\n",
       "         'European': 14,\n",
       "         'Cantonese': 2,\n",
       "         'Persian': 37,\n",
       "         'Vegetarian': 41,\n",
       "         'Portuguese': 16,\n",
       "         'Polish': 4,\n",
       "         'Azerbaijan': 1,\n",
       "         'Russian': 2,\n",
       "         'Bagels': 6,\n",
       "         'Sandwiches': 83,\n",
       "         'Vietnamese': 31,\n",
       "         'Cakes': 14,\n",
       "         'BBQ': 6,\n",
       "         '*London': 1,\n",
       "         'Loves*': 1,\n",
       "         'French': 7,\n",
       "         'Burritos': 6,\n",
       "         'Iranian': 15,\n",
       "         'Noodles': 44,\n",
       "         'Spanish': 8,\n",
       "         'Brazilian': 7,\n",
       "         'food': 7,\n",
       "         'Taiwanese': 1,\n",
       "         'Lunch': 14,\n",
       "         'North': 3,\n",
       "         'Bubble': 2,\n",
       "         'Tea': 2,\n",
       "         \"Panini's\": 3,\n",
       "         'Tex': 4,\n",
       "         'Mex': 4,\n",
       "         'Crepes': 3,\n",
       "         'Street': 4,\n",
       "         'Salads': 8,\n",
       "         'Nepalese': 31,\n",
       "         'Balti': 3,\n",
       "         'Biryani': 6,\n",
       "         'Waffles': 1,\n",
       "         'German': 1,\n",
       "         'Continental': 9,\n",
       "         'Hot': 2,\n",
       "         'Dogs': 2,\n",
       "         'Jerk': 5,\n",
       "         'Tapas': 2,\n",
       "         'Steak': 6,\n",
       "         'Gluten': 3,\n",
       "         'Free': 3,\n",
       "         'Latin': 4,\n",
       "         'Dinner': 3,\n",
       "         'Filipino': 2,\n",
       "         'Indo-Chinese': 1,\n",
       "         'Punjabi': 2,\n",
       "         'Peruvian': 1,\n",
       "         'Ukrainian': 1,\n",
       "         'Singapore': 2,\n",
       "         'Kurdish': 1,\n",
       "         'Subways': 58,\n",
       "         'Kosher': 1,\n",
       "         'Ghanaian': 1,\n",
       "         'Deli': 1,\n",
       "         'Hungarian': 1,\n",
       "         'Roast': 4,\n",
       "         'Dinners': 4})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_lists=[x.split(' ') for x in df2.cuisine]\n",
    "flat_list = [item for sublist in list_lists for item in sublist]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "Counter(flat_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional data prep\n",
    "\n",
    "df3=df3.dropna()\n",
    "\n",
    "chains=['kfc' , 'burger king' , 'subway' , 'pizza hut' , 'papa j']\n",
    "\n",
    "df3['corporation']=['chain' if any(thing in x.lower() for thing in chains) else 'independent' for x in df3.name]\n",
    "      \n",
    "df3[df3.total_reviews>50].to_csv('test.csv')\n",
    "\n",
    "postcodes=[]\n",
    "for thing in df3.pc_prefix:\n",
    "    if 'BR' in thing:\n",
    "        a='BR'\n",
    "    elif 'DA' in thing:\n",
    "        a='DA'\n",
    "    elif 'HA' in thing:\n",
    "        a='HA'\n",
    "    elif 'EN' in thing:\n",
    "        a='EN'\n",
    "    elif 'IG' in thing:\n",
    "        a='IG'\n",
    "    elif 'KT' in thing:\n",
    "        a='KT'\n",
    "    elif 'SM' in thing:\n",
    "        a='SM'\n",
    "    elif 'TW' in thing:\n",
    "        a='TW' \n",
    "    elif 'UB' in thing:\n",
    "        a='UB'\n",
    "    elif 'WC1' in thing:\n",
    "        a='WC1'    \n",
    "    elif 'WC2' in thing:\n",
    "        a='WC2'\n",
    "    elif 'EC1' in thing:\n",
    "        a='EC1'        \n",
    "    elif 'EC2' in thing:\n",
    "        a='EC2'        \n",
    "    elif 'EC3' in thing:\n",
    "        a='EC3'        \n",
    "    elif 'EC4' in thing:\n",
    "        a='EC4' \n",
    "    else:\n",
    "        a=thing\n",
    "    postcodes.append(a)\n",
    "        \n",
    "\n",
    "df3.pc_prefix=postcodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latitude(x):\n",
    "    try:\n",
    "        a=requests.get('http://api.postcodes.io/postcodes/{}'.format(x)).json()['result']['latitude']\n",
    "    except: \n",
    "        a=np.nan\n",
    "    return a\n",
    "        \n",
    "def longitude(x):\n",
    "    try:\n",
    "        a=requests.get('http://api.postcodes.io/postcodes/{}'.format(x)).json()['result']['longitude']\n",
    "    except: \n",
    "        a=np.nan\n",
    "    return a\n",
    "\n",
    "df3['latitude']=df3.postcode.apply(latitude)\n",
    "df3['longitude']=df3.postcode.apply(latitude)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dash Code - Run in Anaconda Prompt for preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using bootstrap\n",
    "\n",
    "import numpy as np\n",
    "import dash\n",
    "import dash_table\n",
    "import dash_core_components as dcc\n",
    "import dash_bootstrap_components as dbc\n",
    "import dash_html_components as html\n",
    "import pandas as pd\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import base64\n",
    "\n",
    "mapbox_access_token = 'pk.eyJ1Ijoiam9leTkxIiwiYSI6ImNqdDA3emFzazA5Y2IzeW56ZHRtN3Q3NG0ifQ.0l4idLnp8xRT8Y_Ipyj7vg'\n",
    "\n",
    "#dataframe for maps and table\n",
    "table_df = pd.read_csv('C:\\\\Users\\\\Joey\\\\Google Drive\\\\5. Python Programs\\\\Webscraping\\\\2.Unfinished\\\\Chicken Dash\\\\takeaway_data.csv',index_col=0)\n",
    "table_df=table_df.rename(columns={'total_reviews':'Reviews ','trending_score':'Trending Score ','overall_rating':'Lifetime Score '})\n",
    "\n",
    "table_df = table_df.sort_values('Trending Score ',ascending=False)\n",
    "table_df=table_df.dropna()\n",
    "\n",
    "zipped=zip(table_df.name\n",
    "           ,table_df.postcode\n",
    "           ,table_df['Lifetime Score '])\n",
    "\n",
    "table_df['text']=[x +', '+y+', Rating: '+str(z) for x,y,z in zipped]\n",
    "\n",
    "#for cuisine selector\n",
    "acceptable=['Chicken','Pizza','Chinese','Italian','Caribbean','Burgers','Kebab','Sandwiches','British','Thai',\n",
    "           'Vietnamese','Japanese','African','Indian','Curry','Mexican','Mediterranean']\n",
    "\n",
    "\n",
    "#Human Theory Logo\n",
    "image_filename = 'C:\\\\Users\\\\Joey\\\\Google Drive\\\\5. Python Programs\\\\Webscraping\\\\2.Unfinished\\\\Chicken Dash\\\\Human-Theory-Typemark-Sunset-Orange_RGB.png'\n",
    "encoded_image = base64.b64encode(open(image_filename, 'rb').read())\n",
    "\n",
    "\n",
    "#top of site navigation bar\n",
    "\n",
    "navbar = dbc.NavbarSimple(\n",
    "    html.A([\n",
    "            html.Img(src='data:image/png;base64,{}'.format(encoded_image.decode()),height='70',\n",
    "             width='140')\n",
    "        ], href='https://www.humantheory.co.uk'),\n",
    "\n",
    "#     brand=\"Human Theory\",\n",
    "#     brand_href=\"https://www.humantheory.co.uk/\",\n",
    "    sticky=\"top\",\n",
    ")\n",
    "\n",
    "\n",
    "#2 containers in one row\n",
    "\n",
    "body = dbc.Container(\n",
    "    [\n",
    "        dbc.Row(\n",
    "            [\n",
    "                dbc.Col(\n",
    "                    [\n",
    "                        html.H2(\"Best in your area\"),\n",
    "                        dcc.Dropdown(id='postSelector',\n",
    "                                     options=[{'label': i, 'value': i} for i in sorted(table_df.pc_prefix.unique())],\n",
    "                                     value='E8',\n",
    "                                     style={'width': '90%'}),\n",
    "\n",
    "#                        dcc.Dropdown(id='cuisineSelector2',\n",
    "#                                     options=[{'label': i, 'value': i} for i in sorted(acceptable)],\n",
    "#                                     value='Chicken',\n",
    "#                                     style={'width': '90%'}),\n",
    "\n",
    "                        html.Div(id='table-container')\n",
    "                    ],\n",
    "                    md=4,\n",
    "                ),\n",
    "                dbc.Col(\n",
    "                    [\n",
    "                       html.H2(\"Takeaway Map of London\"),\n",
    "\n",
    "                       dcc.Dropdown(id='cuisineSelector',\n",
    "                                    options=[{'label': i, 'value': i} for i in sorted(acceptable)],\n",
    "                                    value='Chicken',\n",
    "                                    style={'width': '48%'}),\n",
    "\n",
    "                       dcc.RadioItems(id='corpSelector',\n",
    "                                      options=[{'label': i, 'value': i} for i in ['chain', 'independent']],\n",
    "                                      value='independent',\n",
    "                                      labelStyle={'display': 'inline-block'}),\n",
    "\n",
    "                      dcc.Graph(id='area-shops')\n",
    "                    ]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    className=\"mt-4\",\n",
    ")\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.CERULEAN])\n",
    "\n",
    "app.layout = html.Div([navbar, body])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#callbacks\n",
    "\n",
    "def generate_table(dataframe):\n",
    "    return html.Table(\n",
    "        # Header\n",
    "        [html.Tr([html.Th(col) for col in dataframe.columns])] +\n",
    "\n",
    "        # Body\n",
    "        [html.Tr([\n",
    "        html.Td(dataframe.iloc[i][col]) for col in dataframe.columns\n",
    "        ]) for i in range(len(dataframe))]\n",
    "    )\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('table-container', 'children'),\n",
    "    [dash.dependencies.Input('postSelector', 'value'),\n",
    "     dash.dependencies.Input('cuisineSelector', 'value')])\n",
    "\n",
    "def display_table(postSelector,cuisineSelector):\n",
    "\n",
    "    #df = table_df[(table_df.pc_prefix==postSelector) ][['name',  'Trending Score ', 'Lifetime Score ','Reviews ']].head(10)\n",
    "\n",
    "    df = table_df[(table_df.pc_prefix==postSelector) &\n",
    "                  (table_df.cuisine.str.contains(cuisineSelector, regex=False))][['name',  'Trending Score ', 'Lifetime Score ','Reviews ']].head(10)\n",
    "\n",
    "    return generate_table(df)\n",
    "\n",
    "\n",
    "\n",
    "#map callback\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('area-shops', 'figure'),\n",
    "    [#dash.dependencies.Input('postSelector', 'value'),\n",
    "    dash.dependencies.Input('cuisineSelector', 'value'),\n",
    "    dash.dependencies.Input('corpSelector', 'value')\n",
    "    ])\n",
    "\n",
    "def update_graph(cuisineSelector='Chicken',corpSelector='independent'):\n",
    "\n",
    "\n",
    "    df=table_df[#(table_df.pc_prefix==postSelector) &\n",
    "                (table_df.cuisine.str.contains(cuisineSelector, regex=False))\n",
    "                & (table_df.corporation==corpSelector)\n",
    "    ]\n",
    "\n",
    "    #red points\n",
    "    data = [\n",
    "        go.Scattermapbox(\n",
    "            lat=df[df['Lifetime Score ']<=60]['latitude'],\n",
    "            lon=df[df['Lifetime Score ']<=60]['longitude'],\n",
    "            text = df[df['Lifetime Score ']<=60]['text'],\n",
    "            mode='markers',\n",
    "            marker=dict(size= 13,color = 'red',opacity = 1),\n",
    "            name='shit food',\n",
    "            showlegend=False\n",
    "        ),\n",
    "\n",
    "    #light red\n",
    "        go.Scattermapbox(\n",
    "            lat=df[(df['Lifetime Score ']>60) & (df['Lifetime Score ']<=70)]['latitude'],\n",
    "            lon=df[(df['Lifetime Score ']>60) & (df['Lifetime Score ']<=70)]['longitude'],\n",
    "            text = df[(df['Lifetime Score ']>60) & (df['Lifetime Score ']<=70)]['text'],\n",
    "            mode='markers',\n",
    "            marker=dict(size= 13,color = 'lightsalmon',opacity = 1),\n",
    "            name='bad food',\n",
    "            showlegend=False),\n",
    "\n",
    "\n",
    "    #lawn green\n",
    "\n",
    "        go.Scattermapbox(\n",
    "            lat=df[(df['Lifetime Score ']>70) & (df['Lifetime Score ']<=80)]['latitude'],\n",
    "            lon=df[(df['Lifetime Score ']>70) & (df['Lifetime Score ']<=80)]['longitude'],\n",
    "            text = df[(df['Lifetime Score ']>70) & (df['Lifetime Score ']<=80)]['text'],\n",
    "            mode='markers',\n",
    "            marker=dict(size= 13,color = 'lawngreen',opacity = 1),\n",
    "            name='good food',\n",
    "            showlegend=False),\n",
    "\n",
    "    #green\n",
    "\n",
    "        go.Scattermapbox(\n",
    "            lat=df[df['Lifetime Score ']>80]['latitude'],\n",
    "            lon=df[df['Lifetime Score ']>80]['longitude'],\n",
    "            text = df[df['Lifetime Score ']>80]['text'],\n",
    "            mode='markers',\n",
    "            marker=dict(size= 13,color = 'green',opacity = 1),\n",
    "            name='excellent food',\n",
    "            showlegend=False),\n",
    "    ]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        autosize=False,\n",
    "        width=700,\n",
    "        height=450,\n",
    "        hovermode='closest',\n",
    "        margin=go.layout.Margin(\n",
    "        l=1,\n",
    "        r=1,\n",
    "         b=20,\n",
    "         t=1,\n",
    "#         pad=4\n",
    "    ),\n",
    "        mapbox=go.layout.Mapbox(accesstoken=mapbox_access_token,\n",
    "                                bearing=0,\n",
    "                                center=go.layout.mapbox.Center(lat=51.512324, lon=-0.075366),\n",
    "                                pitch=0,\n",
    "                                zoom=10),\n",
    "    )\n",
    "\n",
    "    return go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
